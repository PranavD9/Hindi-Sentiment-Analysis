{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgNQeuurr8TN"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4yoJVxzvHFn"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "final_words = []\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "\n",
    "\n",
    "# HAPPY EMOTICONS\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# SAD EMOTICONS\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "# ALL EMOTICONS\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLmE4EpY2GBN"
   },
   "outputs": [],
   "source": [
    "class sentiment:\n",
    "    \n",
    "    def __init__(self,tweet):\n",
    "        self.tweet = tweet\n",
    "        self.final_words =[]\n",
    "    \n",
    "    def stock(self):\n",
    "        self.tweet = re.sub(r'\\$\\w*', '', self.tweet)\n",
    "\n",
    "         \n",
    "    def retweet(self):\n",
    "        self.tweet = re.sub(r'^RT[\\s]+', '', self.tweet)\n",
    "        \n",
    "    def hyperlinks(self):\n",
    "        self.tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', self.tweet)\n",
    "\n",
    "        \n",
    "    def hashtag(self):\n",
    "        self.tweet = re.sub(r'#', '', self.tweet)\n",
    "        \n",
    "    def tokenize(self):\n",
    "        tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        tweet_tokens = tokenizer.tokenize(self.tweet)\n",
    "        self.tweet = tweet_tokens\n",
    "        \n",
    "    def algorithm(self):\n",
    "        for word in self.tweet:\n",
    "            if( word not in stopwords_english and\n",
    "                 word not in emoticons and       \n",
    "                   word not in string.punctuation):\n",
    "            \n",
    "                stem_word = stemmer.stem(word)\n",
    "                self.final_words.append(stem_word)\n",
    "                \n",
    "    def all_functions(self):\n",
    "        self.stock()\n",
    "        self.retweet()\n",
    "        self.hyperlinks()\n",
    "        self.hashtag()\n",
    "        self.tokenize()\n",
    "        self.algorithm()\n",
    "        \n",
    "                        \n",
    "                \n",
    "    def getResult(self):\n",
    "        return self.final_words\n",
    "        \n",
    "       \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tweet):\n",
    "    s=sentiment(tweet)\n",
    "    s.all_functions()\n",
    "\n",
    "    final_words = s.getResult()\n",
    "    words = final_words\n",
    "    words_dictionary = dict([word, True] for word in words)\n",
    "    return words_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets_set=[]\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    "\n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 74.4 %\n",
      "Most Informative Features\n",
      "                     via = True              pos : neg    =     37.0 : 1.0\n",
      "                     sad = True              neg : pos    =     28.1 : 1.0\n",
      "                     bam = True              pos : neg    =     25.0 : 1.0\n",
      "                     x15 = True              neg : pos    =     21.0 : 1.0\n",
      "                  welcom = True              pos : neg    =     15.9 : 1.0\n",
      "                   arriv = True              pos : neg    =     15.0 : 1.0\n",
      "                    sick = True              neg : pos    =     14.2 : 1.0\n",
      "                    glad = True              pos : neg    =     13.8 : 1.0\n",
      "                     ugh = True              neg : pos    =     13.0 : 1.0\n",
      "               goodnight = True              pos : neg    =     12.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "model = classifier\n",
    "print('Accuracy is :',accuracy*100,'%') \n",
    "\n",
    "print (classifier.show_most_informative_features(10))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print(classifier.classify(custom_tweet_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ProbDist with 2 samples>\n",
      "neg\n",
      "0.73800282286521\n",
      "0.2619971771347897\n"
     ]
    }
   ],
   "source": [
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print(prob_result)\n",
    "print(prob_result.max())\n",
    "print (prob_result.prob(\"neg\"))\n",
    "print (prob_result.prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"This laptop is brilliant\" \n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print (classifier.classify(custom_tweet_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ProbDist with 2 samples>\n",
      "pos\n",
      "0.18749999999999975\n",
      "0.8124999999999992\n"
     ]
    }
   ],
   "source": [
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result)\n",
    "print (prob_result.max()) \n",
    "print (prob_result.prob(\"neg\")) \n",
    "print (prob_result.prob(\"pos\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "actual_set = defaultdict(set)\n",
    "predicted_set = defaultdict(set)\n",
    "\n",
    "actual_set_cm = []\n",
    "predicted_set_cm = []\n",
    "\n",
    "for index, (feature, actual_label) in enumerate(test_set):\n",
    "    actual_set[actual_label].add(index)\n",
    "    actual_set_cm.append(actual_label)\n",
    "    \n",
    "    predicted_label = classifier.classify(feature)\n",
    "    \n",
    "    predicted_set[predicted_label].add(index)\n",
    "    predicted_set_cm.append(predicted_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Precision : 0.7337164750957854\n",
      "Pos Recall : 0.766\n",
      "Pos F-measure: 0.7495107632093932\n",
      "Neg Precision: 0.7552301255230126\n",
      "Neg Recall: 0.722\n",
      "Neg F-measure: 0.7382413087934561\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    "\n",
    "print('Pos Precision :',precision(actual_set['pos'], predicted_set['pos']))\n",
    "print('Pos Recall :',recall(actual_set['pos'], predicted_set['pos']))\n",
    "print('Pos F-measure:', f_measure(actual_set['pos'], predicted_set['pos']))\n",
    "print('Neg Precision:', precision(actual_set['neg'], predicted_set['neg']))\n",
    "print('Neg Recall:', recall(actual_set['neg'], predicted_set['neg']))\n",
    "print('Neg F-measure:', f_measure(actual_set['neg'], predicted_set['neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<722>278 |\n",
      "pos | 234<766>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |      n      p |\n",
      "    |      e      o |\n",
      "    |      g      s |\n",
      "----+---------------+\n",
      "neg | <36.1%> 13.9% |\n",
      "pos |  11.7% <38.3%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "Accuracy is : 74.4 %\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "with open('finalized_model.pkl','rb') as f:\n",
    "    finalized_model = pickle.load(f)\n",
    "    \n",
    "custom_tweet = \"I loved the movie\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print(finalized_model.classify(custom_tweet_set)) \n",
    "accuracy = classify.accuracy(finalized_model, test_set)\n",
    "print('Accuracy is :',accuracy*100 , '%') \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sentiment Analysis using NLTK.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
